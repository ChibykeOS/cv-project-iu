# -*- coding: utf-8 -*-
"""cv_project_iu.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15Ej2SDyTqlSMtKkxkZ_io2J1f8S1eM00
"""

!nvidia-smi

!pip install ultralytics
from ultralytics import YOLO
print("YOLOv8 installed successfully")

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d andrewmvd/face-mask-detection

!unzip face-mask-detection.zip -d face_mask_dataset

import os

os.listdir("face_mask_dataset")

print("Images:", len(os.listdir("face_mask_dataset/images")))
print("Annotations:", len(os.listdir("face_mask_dataset/annotations")))

import os

base_dir = "yolo_dataset"

paths = [
    f"{base_dir}/images/train",
    f"{base_dir}/images/val",
    f"{base_dir}/labels/train",
    f"{base_dir}/labels/val",
]

for path in paths:
    os.makedirs(path, exist_ok=True)

print("YOLO directory structure created.")

import xml.etree.ElementTree as ET
import os
import shutil
import random

# Paths
img_dir = "face_mask_dataset/images"
ann_dir = "face_mask_dataset/annotations"
yolo_base = "yolo_dataset"

# Class mapping
classes = {
    "with_mask": 0,
    "without_mask": 1,
    "mask_weared_incorrect": 2
}

# Train/val split
split_ratio = 0.8

images = os.listdir(img_dir)
random.shuffle(images)

split_index = int(len(images) * split_ratio)
train_imgs = images[:split_index]
val_imgs = images[split_index:]

def convert_bbox(size, box):
    dw = 1.0 / size[0]
    dh = 1.0 / size[1]
    x = (box[0] + box[1]) / 2.0
    y = (box[2] + box[3]) / 2.0
    w = box[1] - box[0]
    h = box[3] - box[2]
    return x * dw, y * dh, w * dw, h * dh

def process(images, subset):
    for img in images:
        img_path = os.path.join(img_dir, img)
        ann_path = os.path.join(ann_dir, img.replace(".png", ".xml"))

        if not os.path.exists(ann_path):
            continue

        tree = ET.parse(ann_path)
        root = tree.getroot()

        size = root.find("size")
        w = int(size.find("width").text)
        h = int(size.find("height").text)

        label_lines = []

        for obj in root.findall("object"):
            cls = obj.find("name").text
            if cls not in classes:
                continue

            xmlbox = obj.find("bndbox")
            b = (
                float(xmlbox.find("xmin").text),
                float(xmlbox.find("xmax").text),
                float(xmlbox.find("ymin").text),
                float(xmlbox.find("ymax").text)
            )

            bb = convert_bbox((w, h), b)
            label_lines.append(
                f"{classes[cls]} {' '.join(map(str, bb))}"
            )

        if label_lines:
            # Save label
            label_path = f"{yolo_base}/labels/{subset}/{img.replace('.png', '.txt')}"
            with open(label_path, "w") as f:
                f.write("\n".join(label_lines))

            # Copy image
            shutil.copy(
                img_path,
                f"{yolo_base}/images/{subset}/{img}"
            )

# Process datasets
process(train_imgs, "train")
process(val_imgs, "val")

print("XML to YOLO conversion completed.")

!ls yolo_dataset/images/train | head
!ls yolo_dataset/labels/train | head

!cat yolo_dataset/labels/train/$(ls yolo_dataset/labels/train | head -n 1)

# Commented out IPython magic to ensure Python compatibility.
# %%writefile mask_data.yaml
# path: /content/yolo_dataset
# train: images/train
# val: images/val
# 
# nc: 3
# names:
#   0: with_mask
#   1: without_mask
#   2: mask_weared_incorrect
#

!cat mask_data.yaml

#load pretrained model
from ultralytics import YOLO
model = YOLO("yolov8n.pt")

model.train(
    data="mask_data.yaml",
    epochs=50,
    imgsz=640,
    batch=16,
    device=0
)

metrics = model.val()
print(metrics)

from google.colab import files
files.upload()

!ls

#Load trained model
from ultralytics import YOLO

model = YOLO("/content/runs/detect/train/weights/best.pt")

# Predict for video 1
model.predict(
    source="input_video1.mp4",
    save=True,
    conf=0.4,
    name="predict_video1"
)

model.predict(
    source="input_video2.mp4",
    save=True,
    conf=0.4,
    name="predict_video2"
)

!ls runs/detect/

!ls runs/detect/predict_video1/
!ls runs/detect/predict_video2/

import matplotlib.pyplot as plt
from matplotlib.patches import Rectangle

# Create a simple workflow diagram using rectangles
fig, ax = plt.subplots(figsize=(10, 4))

# Boxes
boxes = ["Video Input", "Frame Extraction", "YOLOv8 Inference", "Annotated Output", "Metrics Analysis"]
colors = ["skyblue", "lightgreen", "salmon", "orange", "violet"]
positions = [(0, 0), (2, 0), (4, 0), (6, 0), (8, 0)]

for (x, y), box, color in zip(positions, boxes, colors):
    ax.add_patch(Rectangle((x, y), 1.8, 1, facecolor=color, edgecolor="black"))
    ax.text(x + 0.9, y + 0.5, box, ha='center', va='center', fontsize=12, fontweight='bold')

# Arrows
for x in range(len(boxes)-1):
    ax.annotate("", xy=(positions[x+1][0]-0.1, 0.5), xytext=(positions[x][0]+1.9, 0.5),
                arrowprops=dict(arrowstyle="->", lw=2))

ax.set_xlim(-0.5, 10)
ax.set_ylim(-0.5, 1.5)
ax.axis('off')
plt.title("Graphical Abstract: YOLOv8 Mask Detection Pipeline", fontsize=14, fontweight='bold')
plt.show()

import networkx as nx

G = nx.DiGraph()

# Nodes
G.add_node("Video Input")
G.add_node("Extract Frames")
G.add_node("Preprocess Frames")
G.add_node("YOLOv8 Detection")
G.add_node("Draw Bounding Boxes")
G.add_node("Reassemble Video")
G.add_node("Output Video")

# Edges
edges = [("Video Input", "Extract Frames"),
         ("Extract Frames", "Preprocess Frames"),
         ("Preprocess Frames", "YOLOv8 Detection"),
         ("YOLOv8 Detection", "Draw Bounding Boxes"),
         ("Draw Bounding Boxes", "Reassemble Video"),
         ("Reassemble Video", "Output Video")]
G.add_edges_from(edges)

plt.figure(figsize=(12,6))
pos = nx.spring_layout(G, seed=42, k=1)
nx.draw(G, pos, with_labels=True, node_size=4000, node_color="lightblue", font_size=12, font_weight='bold', arrowsize=20)
plt.title("YOLOv8 Video Processing Flowchart", fontsize=14, fontweight='bold')
plt.show()